# 1. Feature Engineering

### 1.1 Feature normalisation

**Q1. Why do we need to normalise features in the data?**

Different features comprise data on varying scales and standards. When dealing with all the features together, it is necessary to analyse them on a basis of their variance and distribution for better accuracy and interpretation. Normalisation standardises features onto the same scale but keeps their variance.

- Min-Max scales data $wrt.$ the maximum $X_{max}$ and minimum value $X_{min}$

$$
X_{norm} = \frac {X-X_{min}} {X_{max}- X_{min}}
$$

- Z-score returns a standard normal distribution using mean $\mu$ and standard deviation $\sigma$ of the original data

$$
z = \frac {x-\mu}{\sigma}
$$



### 1.2 Categorical feature

**Q2. How to deal with categorical features in data pre-processing?**

Categorical data do not have statistical meaning and thus cannot be quantitatively   processed as numeric. They must be converted to numerical features before being included in most machine learning models.

*Data types: nominal, ordinal, interval/ ratio*

- Ordinal encoding: use ordinal information directly as features.
- One-hot encoding: replace with a high-dimensional sparse array like $[1,0,0,0]$ which indicates the category by ‘1’ and others by ‘0’ respectively.
- Binary encoding: use binary representation of the ordinal data.



**Q3. How to process high-dimensional joint features?**

Combining discrete features can improve fitting in complex relationships. In logistic regression, suppose the feature vector is $X = (x_1, x_2, ..., x_k)$, we have:
$$
Y = sigmoid(\sum_m\sum_nw_{ij}<x_i, x_j>)
$$
where $<x_i, x_j>$ represents the joint features of $x_i, x_j$, which have $m, n$ dimensions respectively. When $m\times n$ is large, we process the joint features by selecting weights on $k$ lower dimensions using matrix decomposition.



**Q4. How to find joint features effectively?**