{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import os \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_for_demonstration(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, depth):\n",
    "        super(lstm_for_demonstration, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, out_dim, depth)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        out, hidden = self.lstm(inputs, hidden)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model_dimension = 8\n",
    "sequence_length = 20\n",
    "batch_size = 1\n",
    "lstm_depth = 1 \n",
    "\n",
    "# random data for input\n",
    "inputs = torch.randn(sequence_length, batch_size, model_dimension)\n",
    "# hidden is actually is a tuple of the initial hidden state and the initial cell state\n",
    "hidden = (torch.randn(lstm_depth, batch_size, model_dimension),\n",
    "         torch.randn(lstm_depth, batch_size, model_dimension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the floating point version of this module:\n",
      "lstm_for_demonstration(\n",
      "  (lstm): LSTM(8, 8)\n",
      ")\n",
      "\n",
      "and now the quantized version:\n",
      "lstm_for_demonstration(\n",
      "  (lstm): DynamicQuantizedLSTM(8, 8)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "float_lstm = lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n",
    "\n",
    "quantized_lstm = torch.quantization.quantize_dynamic(\n",
    "    float_lstm, {nn.LSTM, nn.Linear}, dtype=torch.qint8)\n",
    "    \n",
    "print('Here is the floating point version of this module:')\n",
    "print(float_lstm)\n",
    "print('')\n",
    "print('and now the quantized version:')\n",
    "print(quantized_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (KB): 3.111\n",
      "model:  int8  \t Size (KB): 1.861\n",
      "1.67 times smaller\n"
     ]
    }
   ],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \", label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove(\"temp.p\")\n",
    "    return size\n",
    "\n",
    "f = print_size_of_model(float_lstm, \"fp32\")\n",
    "q = print_size_of_model(quantized_lstm, \"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point FP32\n",
      "1.31 ms ± 47.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "Quantized INT8\n",
      "467 µs ± 4.77 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# compare the performance\n",
    "print(\"Floating point FP32\")\n",
    "%timeit float_lstm.forward(inputs, hidden)\n",
    "\n",
    "print(\"Quantized INT8\")\n",
    "%timeit quantized_lstm.forward(inputs,hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean absolute value of output tensor values in the FP32 model is 0.14986 \n",
      "mean absolute value of output tensor values in the INT8 model is 0.14672\n",
      "mean absolute value of the difference between the output tensors is 0.01802 or 12.02 percent\n"
     ]
    }
   ],
   "source": [
    "# run the float model\n",
    "out1, hidden1 = float_lstm(inputs, hidden)\n",
    "mag1 = torch.mean(abs(out1)).item()\n",
    "print('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n",
    "\n",
    "# run the quantized model\n",
    "out2, hidden2 = quantized_lstm(inputs, hidden)\n",
    "mag2 = torch.mean(abs(out2)).item()\n",
    "print('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n",
    "\n",
    "# compare them\n",
    "mag3 = torch.mean(abs(out1-out2)).item()\n",
    "print('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
